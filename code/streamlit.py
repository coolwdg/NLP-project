import streamlit as st
import os
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import hashlib

# --------- è§£æå¹¶ç»„åˆ TEXT + QAs ---------
def parse_and_merge_blocks(file_obj):
    blocks = []
    current_text = ""
    qa_list = []
    current_question, current_answer = None, None

    lines = file_obj.read().decode("utf-8").splitlines()

    for line in lines:
        line = line.strip()
        if line.startswith("[TEXT]"):
            if current_text and qa_list:
                blocks.append({
                    "text": current_text.strip(),
                    "qas": qa_list
                })
            current_text = line.replace("[TEXT]", "").strip()
            qa_list = []
        elif line.startswith("[QUESTION]"):
            current_question = line.replace("[QUESTION]", "").strip()
        elif line.startswith("[ANSWER]"):
            current_answer = line.replace("[ANSWER]", "").strip()
            if current_question and current_answer:
                qa_list.append({
                    "question": current_question,
                    "answer": current_answer
                })
                current_question, current_answer = None, None

    if current_text and qa_list:
        blocks.append({
            "text": current_text.strip(),
            "qas": qa_list
        })

    return blocks

# --------- è®¡ç®—æ–‡ä»¶hashç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯æ–°æ–‡ä»¶ ---------
def compute_file_hash(file_obj):
    file_obj.seek(0)
    file_bytes = file_obj.read()
    file_obj.seek(0)
    return hashlib.md5(file_bytes).hexdigest()

# --------- æ„å»ºå‘é‡ç´¢å¼• ---------
def build_combined_index(blocks, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
    model = SentenceTransformer(model_name)
    combined_texts = []

    for block in blocks:
        combined = block['text']
        for qa in block['qas']:
            combined += f"ã€‚é—®é¢˜ï¼š{qa['question']} ç­”æ¡ˆï¼š{qa['answer']}"
        combined_texts.append(combined)

    embeddings = model.encode(combined_texts, convert_to_numpy=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    return index, model

# --------- ç›¸ä¼¼åº¦æ£€ç´¢ ---------
def retrieve_combined_blocks(query, model, index, blocks, top_k=5):
    query_vec = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)

    results = []
    for i, idx in enumerate(indices[0]):
        block = blocks[idx]
        similarity = 1 / (1 + distances[0][i])
        results.append((similarity, block['text'], block['qas']))

    return results

# --------- è°ƒç”¨å¤§æ¨¡å‹ ---------
def ask_doubao(context, user_question):
    client = OpenAI(
        base_url="",
        api_key=""
    )

    completion = client.chat.completions.create(
        model="doubao-pro-256k-241115",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåªèƒ½åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”ï¼š'æˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ï¼Œå› ä¸ºæä¾›çš„å†…å®¹ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ã€‚'"},
            {"role": "user", "content": f"å·²çŸ¥å†…å®¹å¦‚ä¸‹ï¼š\n{context}\n\nè¯·åŸºäºä¸Šè¿°å†…å®¹å›ç­”ï¼š{user_question}"},
        ],
    )
    return completion.choices[0].message.content

# --------- åˆå§‹åŒ– Streamlit çŠ¶æ€ ---------
st.set_page_config(page_title="é—®ç­”åŠ©æ‰‹", layout="wide")
st.title("ğŸ“˜ æ–‡æœ¬é—®ç­”æ™ºèƒ½åŠ©æ‰‹")

if 'blocks' not in st.session_state:
    st.session_state.blocks = None
if 'index' not in st.session_state:
    st.session_state.index = None
if 'model' not in st.session_state:
    st.session_state.model = None
if 'file_hash' not in st.session_state:
    st.session_state.file_hash = None

# --------- ä¸Šä¼ æ–‡ä»¶å¹¶ç¼“å­˜å¤„ç†ç»“æœ ---------
uploaded_file = st.file_uploader("è¯·ä¸Šä¼ åŒ…å« [TEXT]/[QUESTION]/[ANSWER] æ ‡è®°çš„æ–‡æœ¬æ–‡ä»¶", type=["txt"])

if uploaded_file:
    current_hash = compute_file_hash(uploaded_file)

    if st.session_state.file_hash != current_hash:
        with st.spinner("â³ æ­£åœ¨è§£ææ–‡ä»¶å¹¶æ„å»ºç´¢å¼•..."):
            blocks = parse_and_merge_blocks(uploaded_file)
            index, model = build_combined_index(blocks)

            st.session_state.blocks = blocks
            st.session_state.index = index
            st.session_state.model = model
            st.session_state.file_hash = current_hash

        st.success(f"âœ… æˆåŠŸè½½å…¥ {len(blocks)} ä¸ªæ–‡æ¡£å—ã€‚å¯ä»¥å¼€å§‹æé—®äº†ï¼")
    else:
        st.info("ğŸ“ æ–‡ä»¶æœªæ›´æ”¹ï¼Œä½¿ç”¨å·²ç¼“å­˜çš„æ¨¡å‹å’Œç´¢å¼•ã€‚")

# --------- æé—® & æ˜¾ç¤ºç»“æœ ---------
if st.session_state.blocks and st.session_state.index and st.session_state.model:
    user_question = st.text_input("ğŸ’¬ è¾“å…¥ä½ çš„é—®é¢˜ï¼š")

    if user_question:
        with st.spinner("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹..."):
            results = retrieve_combined_blocks(
                user_question,
                st.session_state.model,
                st.session_state.index,
                st.session_state.blocks,
                top_k=5
            )

        # æ‹¼æ¥ä¸Šä¸‹æ–‡å†…å®¹
        all_context_parts = []
        st.subheader("ğŸ“š åŒ¹é…çš„æ–‡æœ¬æ®µè½ï¼ˆTop 5ï¼‰")
        for i, (score, text, qas) in enumerate(results[:5], 1):
            st.markdown(f"**{i}. åŒ¹é…åº¦ï¼š{score:.4f}**")
            st.markdown(f"ğŸ“˜ {text}")
            context_part = f"æ®µè½ {i}ï¼š{text}"
            for qa in qas:
                context_part += f"\nQ: {qa['question']}\nA: {qa['answer']}"
            all_context_parts.append(context_part)

        full_context = "\n\n".join(all_context_parts)

        with st.spinner("ğŸ¤– æ­£åœ¨å‘å¤§æ¨¡å‹è¯·æ±‚ç­”æ¡ˆ..."):
            response = ask_doubao(full_context, user_question)

        st.subheader("ğŸ§  å¤§æ¨¡å‹å›ç­”")
        st.markdown(response)
